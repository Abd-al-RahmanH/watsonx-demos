{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transript = \"\"\"嗨您好\n",
    "我這裡是信安保金代\n",
    "我在5月30號的時候\n",
    "有幫我的客戶投保了一張傷害險\n",
    "但是我在6月12號的時候\n",
    "收到一張照會單\n",
    "跟我說\n",
    "因為累積同業傷害險的保額\n",
    "超過該個險種的個人年收入的倍數\n",
    "所以我在賦約那個Y500R2的\n",
    "那個限額投保275萬\n",
    "叫我要補上契約變更書\n",
    "調降那個賦約裡面的投保保額\n",
    "想跟你確認一下這件事是怎麼了\n",
    "好您好\n",
    "那個我想先跟你核對一下身份\n",
    "請問您在信安保金代的銷售人編號是\n",
    "AR524403\n",
    "好的\n",
    "那再跟您請教一下\n",
    "這一張保單的編號是\n",
    "MA44632\n",
    "好的\n",
    "我看一下這張保單的狀況\n",
    "嗯\n",
    "這邊我們在系統上看到\n",
    "因為您的客戶已經在其他保險公司同業\n",
    "投保過了傷害險\n",
    "所以才會有這個問題\n",
    "可是按照我客戶的年收入\n",
    "他可以投保的最高理賠額度是975萬\n",
    "嗯是的沒錯\n",
    "但是我們在授權公會上面\n",
    "看到他在同業已經投保了700萬的傷害險\n",
    "所以在我們這邊只能提供275萬傷害險的額度\n",
    "那這邊可能要請您跟客戶確認一下\n",
    "他是不是已經在哪一家保險公司有投保過\n",
    "嗨那請問一下\n",
    "你這邊有辦法透露是哪一家保險公司嗎\n",
    "嗯不好意思\n",
    "沒有辦法\n",
    "因為根據各自法的關係\n",
    "我們不能透露客戶詳細的投保資訊\n",
    "可能要請您客戶這邊\n",
    "自己找他原本的投保公司來申請投保資訊\n",
    "或者是投保記錄\n",
    "好的謝謝你\n",
    "我來跟我的客戶確認一下\n",
    "好謝謝\n",
    "拜拜\n",
    "拜拜\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = {\n",
    "    \"url\"    : \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\" : api_key\n",
    "}\n",
    "\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD:\"greedy\",\n",
    "    GenParams.MAX_NEW_TOKENS:500,\n",
    "    GenParams.MIN_NEW_TOKENS:1,\n",
    "    GenParams.TOP_K:50,\n",
    "    GenParams.TOP_P:1,\n",
    "    GenParams.STOP_SEQUENCES:[\"<eof>\"],\n",
    "}\n",
    "\n",
    "llm = Model(\"meta-llama/llama-2-70b-chat\",creds, params,project_id)\n",
    "# llm = Model(\"ibm/granite-13b-chat-v2\",creds, params,project_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import pathlib\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader, PyMuPDFLoader, PyPDFium2Loader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from langchain_ibm import WatsonxLLM\n",
    "\n",
    "\n",
    "from typing import Literal, Optional, Any\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, Milvus, FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from genai.client import Client\n",
    "from genai.credentials import Credentials\n",
    "from genai.schema import TextGenerationParameters, TextGenerationReturnOptions\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationHAPOutput,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "client = Client(credentials=Credentials.from_env())\n",
    "\n",
    "model = LangChainInterface(\n",
    "    model_id=\"meta-llama/llama-2-70b-chat\",\n",
    "    # model_id=\"mistralai/mixtral-8x7b-instruct-v0-1\",\n",
    "    client=Client(credentials=Credentials.from_env()),\n",
    "    parameters=TextGenerationParameters(\n",
    "        decoding_method=DecodingMethod.GREEDY,\n",
    "        max_new_tokens=2000,\n",
    "        min_new_tokens=1,\n",
    "        # temperature=0.05,\n",
    "        top_k=50,\n",
    "        top_p=1,\n",
    "        # stop_sequences=['```'],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "對話紀錄:\n",
       "================\n",
       "\n",
       "對話人員:\n",
       "---------------\n",
       "\n",
       "* Speaker 1: 信安保金代的銷售人員 (AR524403)\n",
       "* Speaker 2: 全球人壽照會客服人員\n",
       "\n",
       "主要訴求:\n",
       "---------------\n",
       "\n",
       "* 信安保金代的客戶投保了一張傷害險，但是在其他保險公司已經投保過了傷害險，導致現在的投保額度僅能提供275萬\n",
       "* 客戶的年收入可以投保的最高理賠額度是975萬，但是因為已經投保過傷害險，所以只能提供275萬的額度\n",
       "* 銷售人員需要跟客戶確認一下他是不是已經在哪一家保險公司有投保過\n",
       "* 全球人壽照會客服人員不能透露客戶詳細的投保資訊，可能要請您客戶這邊\n",
       "* 銷售人員需要跟客戶確認一下他的投保記錄或者是投保公司\n",
       "\n",
       "識別對方的情緒:\n",
       "-------------------------\n",
       "\n",
       "* 銷售人員的情緒不明確，但是可能是對客戶的調查和確認\n",
       "* 全球人壽照會客服人員的情緒也不明確，但是可能是對客戶的調查和確認\n",
       "\n",
       "跟進事項:\n",
       "--------------\n",
       "\n",
       "* 銷售人員需要跟客戶確認一下他是不是已經在哪一家保險公司有投保過\n",
       "* 銷售人員需要跟客戶確認一下他的投保記錄或者是投保公司\n",
       "* 全球人壽照會客服人員需要跟銷售人員聯繫，以確保客戶的投保資訊是正確的\n",
       "\n",
       "識別產品:\n",
       "--------------\n",
       "\n",
       "* 傷害險\n",
       "\n",
       "提取銷售人員編號:\n",
       "---------------------------\n",
       "\n",
       "* AR524403\n",
       "\n",
       "提取保單編號:\n",
       "---------------------\n",
       "\n",
       "* MA44632\n",
       "\n",
       "目標日期:\n",
       "-------------\n",
       "\n",
       "* 未提及\n",
       "\n",
       "請注意，這只是一個機器翻譯的結果，可能存在一些偏差或錯誤。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, HTML, clear_output\n",
    "\n",
    "prompt = f\"\"\"[INST]你是人壽客服中心分析人員，請總結以下對話紀錄，並提取關鍵信息。\n",
    "    -識別對話人員身分，並在稍後的紀錄恰當描述對方身分。\n",
    "    -請適當指稱對話人員，對話的人不一定是客戶，客戶在人壽行業有特殊定義。\n",
    "    -使用\"speaker 1\"指稱，不要用“客戶”這個指稱。\n",
    "    -speaker 2是全球人壽照會客服人員。\n",
    "    <<SYS>>\n",
    "    對話紀錄: `{transript}`\n",
    "    <</SYS>>\n",
    "    -請生成對話紀錄的總結。\n",
    "    -識別對方的主要訴求，推薦跟進事項。\n",
    "    -識別這個對話是關於什麼產品，提取銷售人員編號，提取保單編號。\n",
    "    -識別對方的情緒。\n",
    "    -跟進事項請明確(誰跟進，需要跟進的任務，目標日期)。\n",
    "    -請生成markdown，適當分行顯示。\n",
    "    [/INST]生成結果 in markdown:```\"\"\"\n",
    "\n",
    "    # 格式:`\n",
    "    # 對話紀錄總結：\n",
    "\n",
    "    # 對話人員身分：\n",
    "\n",
    "    # 主要訴求：\n",
    "\n",
    "    # 產品：\n",
    "\n",
    "    # 保單編號：\n",
    "\n",
    "    # 銷售人員編號：\n",
    "\n",
    "    # 情緒：\n",
    "\n",
    "    # 跟進事項：\n",
    "    # `\n",
    "\n",
    "answer = model.invoke(prompt)\n",
    "\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildprompt(text):\n",
    "    prompt = f\"\"\"[INST]\n",
    "    you are call center specialist, need to summarize the transcript in 200 words:\n",
    "    -generate summary as json.\n",
    "    -base on the summary json format provided.\n",
    "    -dont generate note.\n",
    "    -show json only.\n",
    "    -rating in 5 class (good, bad, )\n",
    "    -end with <eof>\n",
    "    transcript: `{text}`\n",
    "    summary json format:`{{\n",
    "        总结:,\n",
    "        意图:,\n",
    "        产品:,\n",
    "        情绪:,\n",
    "    }}`\n",
    "    [/INST]summary json:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildpromptgetlabels(text):\n",
    "    prompt = f\"\"\"[INST]\n",
    "    you are call center specialist, need to label the transcript in 200 words:\n",
    "    -generate summary as json.\n",
    "    -base on the summary json format provided.\n",
    "    -dont generate note.\n",
    "    -dont generate explaination.\n",
    "    -show json only.\n",
    "    -generate 10 labels for the user and product make most sense.\n",
    "    -end with <eof>\n",
    "    transcript: `{text}`\n",
    "    summary json format:`{{\n",
    "        labels:\n",
    "    }}`\n",
    "    [/INST]summary json:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = 1728970632164929538\n",
    "conversation = pd.read_csv('conversation_log.csv')\n",
    "subset = conversation[conversation['sid']==sid]\n",
    "subset = subset[['role','conversation']]\n",
    "subset.head()\n",
    "ajson = subset.to_json(orient='records',force_ascii=False)\n",
    "# ajson\n",
    "print(f\"\\n客户对话{sid}分析\")\n",
    "prompt = buildprompt(ajson[:4000])\n",
    "for response in llm.generate_text([prompt]):\n",
    "    print(response)\n",
    "print(f\"\\n客户对话{sid}标签\")\n",
    "prompt = buildpromptgetlabels(ajson[:4000])\n",
    "for response in llm.generate_text([prompt]):\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = pd.read_csv('conversation_log.csv')\n",
    "sids = conversation['sid'].unique()\n",
    "for sid in sids:\n",
    "    subset = conversation[conversation['sid']==sid]\n",
    "    subset = subset[['role','conversation']]\n",
    "    # subset.head()\n",
    "    # ajson = subset.to_csv()\n",
    "    # print(ajson)\n",
    "    ajson = subset.to_json(orient='records',force_ascii=False)\n",
    "    # ajson\n",
    "    # print(f\"\\n客户对话{sid}分析\")\n",
    "    # prompt = buildprompt(ajson[:4000])\n",
    "    # for response in llm.generate_text([prompt]):\n",
    "    #     print(response)\n",
    "    print(f\"\\n客户对话{sid}标签\")\n",
    "    prompt = buildpromptgetlabels(ajson[:4000])\n",
    "    for response in llm.generate_text([prompt]):\n",
    "        print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
