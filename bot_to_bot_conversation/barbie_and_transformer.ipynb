{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ibm_watson_machine_learning as wml\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UdbyUSRd6424vys5TlllcXpBRIncXyCqCFOBPGKdrH4C\n",
      "f86be7bf-6c82-492b-88cc-0297c1afc730\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "print(api_key)\n",
    "print(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_credentials = {\n",
    "    \"url\"    : \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\" : api_key\n",
    "}\n",
    "\n",
    "model_id    = ModelTypes.MPT_7B_INSTRUCT2\n",
    "gen_parms   = None\n",
    "project_id  = project_id\n",
    "space_id    = None\n",
    "verify      = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Let's debate----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-----Let's debate----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cycles = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pros_params = {\n",
    "    GenParams.DECODING_METHOD:\"sample\",\n",
    "    GenParams.MAX_NEW_TOKENS:200,\n",
    "    GenParams.MIN_NEW_TOKENS:1,\n",
    "    GenParams.TEMPERATURE:0.7,\n",
    "    GenParams.TOP_K:50,\n",
    "    GenParams.TOP_P:1,\n",
    "}\n",
    "\n",
    "cons_params = {\n",
    "    GenParams.DECODING_METHOD:\"sample\",\n",
    "    GenParams.MAX_NEW_TOKENS:200,\n",
    "    GenParams.MIN_NEW_TOKENS:1,\n",
    "    GenParams.TEMPERATURE:0.7,\n",
    "    GenParams.TOP_K:50,\n",
    "    GenParams.TOP_P:1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_id': 'ibm/mpt-7b-instruct2', 'label': 'mpt-7b-instruct2', 'provider': 'IBM', 'source': 'Hugging Face', 'short_description': 'MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by IBM.', 'long_description': 'MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference. These architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing positional embeddings with Attention with Linear Biases (ALiBi).', 'task_ids': ['summarization', 'classification', 'generation', 'extraction'], 'tasks': [{'id': 'question_answering', 'ratings': {'quality': 2}}, {'id': 'summarization', 'ratings': {'quality': 3}}, {'id': 'retrieval_augmented_generation', 'ratings': {'quality': 1}}, {'id': 'classification', 'ratings': {'quality': 3}}, {'id': 'extraction', 'ratings': {'quality': 3}}], 'model_limits': {'max_sequence_length': 2048}, 'limits': {'lite': {'call_time': '5m0s', 'max_output_tokens': 500}, 'v2-professional': {'call_time': '5m0s', 'max_output_tokens': 500}, 'v2-standard': {'call_time': '5m0s', 'max_output_tokens': 500}}, 'min_shot_size': 0, 'tier': 'class_1', 'number_params': '7b'}\n",
      "{'model_id': 'google/flan-t5-xxl', 'label': 'flan-t5-xxl-11b', 'provider': 'Google', 'source': 'Hugging Face', 'short_description': 'flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.', 'long_description': 'flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned LAnguage Net (FLAN) with instructions for better zero-shot and few-shot performance.', 'task_ids': ['question_answering', 'summarization', 'retrieval_augmented_generation', 'classification', 'generation', 'extraction'], 'tasks': [{'id': 'question_answering', 'ratings': {'quality': 4}}, {'id': 'summarization', 'ratings': {'quality': 4}}, {'id': 'retrieval_augmented_generation', 'ratings': {'quality': 3}}, {'id': 'classification', 'ratings': {'quality': 4}}, {'id': 'extraction', 'ratings': {'quality': 4}}], 'model_limits': {'max_sequence_length': 4096}, 'limits': {'lite': {'call_time': '5m0s', 'max_output_tokens': 700}, 'v2-professional': {'call_time': '5m0s', 'max_output_tokens': 700}, 'v2-standard': {'call_time': '5m0s', 'max_output_tokens': 700}}, 'min_shot_size': 0, 'tier': 'class_2', 'number_params': '11b'}\n"
     ]
    }
   ],
   "source": [
    "creds = my_credentials\n",
    "\n",
    "pros_model = Model(ModelTypes.MPT_7B_INSTRUCT2,\n",
    "                   creds,\n",
    "                   pros_params, project_id, space_id, verify\n",
    ")\n",
    "print(pros_model.get_details())\n",
    "\n",
    "\n",
    "cons_model = Model(ModelTypes.FLAN_T5_XXL,\n",
    "                   creds,\n",
    "                   cons_params, project_id, space_id, verify\n",
    ")\n",
    "print(cons_model.get_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_q = \"Barbie is better than Transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion: Barbie is a classic toy, and it has been loved by children for decades. Although Transformer has become popular recently, Barbie is still the first choice for many children, especially girls. Barbie has the advantages of being a lady and a doll, which are more attractive to girls. In addition, many parents think Barbie is safer than Transformer. Barbie is not easy to break, and it is not easy to swallow, which is more suitable for children.\n",
      "\n",
      "Pros: \n",
      "1. Barbie is more suitable for children. It is safer and easier to swallow.\n",
      "2. Barbie can be the first choice for children. It can cultivate children's interests in dolls.\n",
      "3. Barbie is an attractive toy. It can cultivate children's interests in dolls.\n",
      "4. Barbie can cultivate children's interests in ladies. It can cultivate children's interests in women.\n",
      "5. Barbie can cultivate children's interests in fashion. It can cultivate children's interests in clothes.\n",
      "6. Barbie can cultivate children's interests in fashion. It can cultivate children's interests in beauty.\n",
      "7. Barbie can cultivate children's interests in fashion. It can cultivate children's interests in ladies.\n",
      "8. Barbie can cultivate children's interests in fashion. It can cultivate children's interests in beauty.\n",
      "9. Barbie can cultivate children's interests in fashion. It can cultivate children's interests in ladies.\n",
      "10. Barbie can\n",
      "\n",
      "Cons: Barbie is a classic toy, and it has been loved by children for decades.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = f\"Motion: {cons_q}\\n\"\n",
    "print(history)\n",
    "\n",
    "for x in range(1):\n",
    "    pros_response = pros_model.generate_text(f\"\"\"as an analyst, analyis the following conversation, dont repeat the points in the history, generate opinion with supporting argument in 100 words\n",
    "    ###\n",
    "    history: {history}\n",
    "    ###\n",
    "    Pros: \"\"\")\n",
    "    pros_a = pros_response\n",
    "    print(f\"Pros: {pros_a}\\n\")\n",
    "    history += f\"Pros: {pros_a}\\n\"\n",
    "\n",
    "\n",
    "    cons_response = cons_model.generate_text(f\"\"\"as an analyst, analyis the following conversation, dont repeat the points in the history, generate opinion with supporting argument in 100 words\n",
    "    ###\n",
    "    history: {history}\n",
    "    ###\n",
    "    Cons: \"\"\")\n",
    "    cons_a = cons_response\n",
    "    print(f\"Cons: {cons_a}\\n\")\n",
    "    history += f\"Cons: {cons_a}\\n\"\n",
    "\n",
    "    # print(history)\n",
    "\n",
    "    cons_q = cons_a\n",
    "    # time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
