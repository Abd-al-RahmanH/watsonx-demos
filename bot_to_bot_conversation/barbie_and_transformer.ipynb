{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, ibm_watson_machine_learning as wml\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UdbyUSRd6424vys5TlllcXpBRIncXyCqCFOBPGKdrH4C\n",
      "f86be7bf-6c82-492b-88cc-0297c1afc730\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "print(api_key)\n",
    "print(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_credentials = {\n",
    "    \"url\"    : \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\" : api_key\n",
    "}\n",
    "\n",
    "model_id    = ModelTypes.LLAMA_2_13B_CHAT\n",
    "gen_parms   = None\n",
    "project_id  = project_id\n",
    "space_id    = None\n",
    "verify      = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Let's debate----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n-----Let's debate----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cycles = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pros_params = {\n",
    "    GenParams.DECODING_METHOD:\"sample\",\n",
    "    GenParams.MAX_NEW_TOKENS:200,\n",
    "    GenParams.MIN_NEW_TOKENS:1,\n",
    "    GenParams.TEMPERATURE:0.7,\n",
    "    GenParams.TOP_K:50,\n",
    "    GenParams.TOP_P:1,\n",
    "}\n",
    "\n",
    "cons_params = {\n",
    "    GenParams.DECODING_METHOD:\"sample\",\n",
    "    GenParams.MAX_NEW_TOKENS:200,\n",
    "    GenParams.MIN_NEW_TOKENS:1,\n",
    "    GenParams.TEMPERATURE:0.7,\n",
    "    GenParams.TOP_K:50,\n",
    "    GenParams.TOP_P:1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_id': 'meta-llama/llama-2-13b-chat', 'label': 'llama-2-13b-chat', 'provider': 'Meta', 'source': 'Hugging Face', 'short_description': 'Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.', 'long_description': 'Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.', 'task_ids': ['question_answering', 'summarization', 'retrieval_augmented_generation', 'classification', 'generation', 'code', 'extraction'], 'tasks': [{'id': 'question_answering', 'ratings': {'quality': 4}}, {'id': 'summarization', 'ratings': {'quality': 3}}, {'id': 'retrieval_augmented_generation', 'ratings': {'quality': 4}}, {'id': 'classification', 'ratings': {'quality': 4}}, {'id': 'generation'}, {'id': 'code'}, {'id': 'extraction', 'ratings': {'quality': 4}}], 'model_limits': {'max_sequence_length': 4096}, 'limits': {'lite': {'call_time': '5m0s', 'max_output_tokens': 2048}, 'v2-professional': {'call_time': '5m0s', 'max_output_tokens': 4096}, 'v2-standard': {'call_time': '5m0s', 'max_output_tokens': 4096}}, 'min_shot_size': 1, 'tier': 'class_2', 'number_params': '13b'}\n",
      "{'model_id': 'meta-llama/llama-2-13b-chat', 'label': 'llama-2-13b-chat', 'provider': 'Meta', 'source': 'Hugging Face', 'short_description': 'Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.', 'long_description': 'Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.', 'task_ids': ['question_answering', 'summarization', 'retrieval_augmented_generation', 'classification', 'generation', 'code', 'extraction'], 'tasks': [{'id': 'question_answering', 'ratings': {'quality': 4}}, {'id': 'summarization', 'ratings': {'quality': 3}}, {'id': 'retrieval_augmented_generation', 'ratings': {'quality': 4}}, {'id': 'classification', 'ratings': {'quality': 4}}, {'id': 'generation'}, {'id': 'code'}, {'id': 'extraction', 'ratings': {'quality': 4}}], 'model_limits': {'max_sequence_length': 4096}, 'limits': {'lite': {'call_time': '5m0s', 'max_output_tokens': 2048}, 'v2-professional': {'call_time': '5m0s', 'max_output_tokens': 4096}, 'v2-standard': {'call_time': '5m0s', 'max_output_tokens': 4096}}, 'min_shot_size': 1, 'tier': 'class_2', 'number_params': '13b'}\n"
     ]
    }
   ],
   "source": [
    "creds = my_credentials\n",
    "\n",
    "pros_model = Model(ModelTypes.LLAMA_2_13B_CHAT,\n",
    "                   creds,\n",
    "                   pros_params, project_id, space_id, verify\n",
    ")\n",
    "print(pros_model.get_details())\n",
    "\n",
    "\n",
    "cons_model = Model(ModelTypes.LLAMA_2_13B_CHAT,\n",
    "                   creds,\n",
    "                   cons_params, project_id, space_id, verify\n",
    ")\n",
    "print(cons_model.get_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_q = \"Barbie is better than Transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motion: Barbie is better than Transformer\n",
      "\n",
      "Pros:\n",
      "\n",
      "Barbie has stood the test of time, remaining a beloved toy for multiple generations, while Transformers have had a more limited appeal. Barbie's versatility in terms of dress-up and imaginative play makes her a more engaging toy for young girls.\n",
      "\n",
      "Cons: \n",
      "Transformers' ability to change form and adapt to different situations may be more appealing to boys who enjoy action-oriented play. Additionally, Transformers have a wider range of characters and storylines, which may be more engaging for some children.\n",
      "\n",
      "Overall, Barbie and Transformers cater to different preferences, making both toys valuable in their own ways. However, Barbie's enduring popularity and versatility make her a better toy for young girls.Cons:\n",
      "\n",
      "\n",
      "While Transformers have a wider range of characters and storylines, which may be more engaging for some children, Barbie's versatility in terms of dress-up and imaginative play makes her a more engaging toy for young girls. Additionally, Barbie's enduring popularity for multiple generations is a testament to her timeless appeal, making her a better investment for parents looking for a toy that will stand the test of time."
     ]
    }
   ],
   "source": [
    "history = f\"Motion: {cons_q}\\n\"\n",
    "print(history)\n",
    "\n",
    "for x in range(20):\n",
    "    pros_response = f\"\"\"[INST]as an analyst, analyis the following conversation, dont repeat the points in the history, generate opinion with supporting argument in 100 words\n",
    "    <<SYS>>\n",
    "    history: {history}\n",
    "    <<SYS>>\n",
    "    [/INST]\n",
    "    Pros: \"\"\"\n",
    "    # pros_a = pros_response\n",
    "    pros_a = \"\"\n",
    "    generated_response = pros_model.generate_text_stream(prompt=pros_response)\n",
    "    print(\"Pros:\")\n",
    "    for chunk in generated_response:\n",
    "        print(chunk, end='')\n",
    "        pros_a += chunk\n",
    "    # print(f\"Pros: {pros_a}\\n\")\n",
    "    history += f\"Pros: {pros_a}\\n\"\n",
    "\n",
    "\n",
    "    cons_response = f\"\"\"[INST]as an analyst, analyis the following conversation, dont repeat the points in the history, generate opinion with supporting argument in 100 words\n",
    "    <<SYS>>\n",
    "    history: {history}\n",
    "    <<SYS>>\n",
    "    [/INST]\n",
    "    Cons: \"\"\"\n",
    "    cons_a = \"\"\n",
    "    generated_response = cons_model.generate_text_stream(prompt=cons_response)\n",
    "    print(f\"Cons:\")\n",
    "    for chunk in generated_response:\n",
    "        print(chunk, end='')\n",
    "        cons_a += chunk\n",
    "    history += f\"Cons: {cons_a}\\n\"\n",
    "\n",
    "    # print(history)\n",
    "\n",
    "    cons_q = cons_a\n",
    "    # time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
