{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "\n",
    "from genai.model import GenerateParams\n",
    "from genai import Credentials\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.document_loaders import PDFMinerLoader\n",
    "from langchain import LLMChain\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GENAI_KEY\",None)\n",
    "api_endpoint = os.getenv(\"GENAI_API\",None)\n",
    "\n",
    "creds = Credentials(api_key,api_endpoint)\n",
    "\n",
    "params = GenerateParams(\n",
    "    decoding_method=\"greedy\",\n",
    "    temperature=0.05,\n",
    "    max_new_tokens=300,\n",
    "    min_new_tokens=15,\n",
    "    repetition_penalty=2,\n",
    ")\n",
    "\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size= chunk_size, \n",
    "                                               chunk_overlap=chunk_overlap,\n",
    "                                               separators=[\"\\n\\n\" ,\"(?=>\\.)\"\n",
    "                                                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = HuggingFaceEmbeddings()\n",
    "# embeddings = HuggingFaceInstructEmbeddings(\n",
    "#             model_name=\"hkunlp/instructor-large\"\n",
    "#         )\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "digest the pdf to vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when your document number increase, you would like to do filtering to narrow down the search scope, and you can leverage metadata to do so, firstly, you need to tag your document with corresponding metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reset the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", None)\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", None)\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", None)\n",
    "print(COLLECTION_NAME)\n",
    "print(MILVUS_HOST)\n",
    "print(MILVUS_PORT)\n",
    "\n",
    "from pymilvus import connections, utility\n",
    "\n",
    "connections.connect(COLLECTION_NAME, host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "try:\n",
    "    if utility.has_collection(COLLECTION_NAME):\n",
    "        utility.drop_collection(COLLECTION_NAME)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LangChainInterface(model=\"bigscience/mt0-xxl\",credentials=creds,params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"category.txt\", \"r\")\n",
    "categories = file.readlines()\n",
    "\n",
    "print(len(categories))\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "docs = []\n",
    "\n",
    "for category in categories:\n",
    "    for path in Path('../../menu/'+category).rglob('*.pdf'):\n",
    "        loader = PDFMinerLoader('../../menu/'+category+'/'+ path.name)\n",
    "        data = loader.load()\n",
    "        for doc in data:\n",
    "            doc.metadata['product'] = category\n",
    "        docs += text_splitter.split_documents(data)\n",
    "\n",
    "# prompt_template = \"Summerize the message \\\"{content}\\\"\"\n",
    "\n",
    "# llm_chain = LLMChain(\n",
    "#     llm=llm,\n",
    "#     prompt=PromptTemplate.from_template(prompt_template)\n",
    "# )\n",
    "\n",
    "db = Milvus.from_documents(docs,embeddings,\n",
    "                        collection_name = COLLECTION_NAME,\n",
    "                        connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", None)\n",
    "MILVUS_HOST = os.getenv(\"MILVUS_HOST\", None)\n",
    "MILVUS_PORT = os.getenv(\"MILVUS_PORT\", None)\n",
    "print(COLLECTION_NAME)\n",
    "print(MILVUS_HOST)\n",
    "print(MILVUS_PORT)\n",
    "db = Milvus(\n",
    "    embedding_function = embeddings,\n",
    "    collection_name = COLLECTION_NAME,\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT},\n",
    "    drop_old = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"sample.txt\", \"r\")\n",
    "questions = file.readlines()\n",
    "\n",
    "print(len(questions))\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can do filtering on similarity_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Use three sentences maximum and keep the answer as concise as possible. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "# result = qa_chain1({\"query\": \"how to clean the refregirator?\"})\n",
    "# result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for query in questions:\n",
    "    print(\"Q:\"+query)\n",
    "    resultdocs = db.similarity_search(query, k=3)\n",
    "    for doc in resultdocs:\n",
    "        print(doc)\n",
    "    answerss = chain.run(input_documents=resultdocs, question=query)\n",
    "    print(\"A(SS):\"+answerss)\n",
    "    result = qa_chain({\"query\": query})\n",
    "    answerr = result[\"result\"]\n",
    "    print(\"A(R):\"+answerr)\n",
    "    new_record = pd.DataFrame([{'Question':query, \n",
    "                                'Answer_Similarity_Search':answerss,\n",
    "                                'Answer_Retriever':answerr,\n",
    "                                }])\n",
    "    data = pd.concat([data,new_record],ignore_index=True)\n",
    "\n",
    "    # sources = result[\"source_documents\"]\n",
    "    # for src in sources:\n",
    "    #     print(src)\n",
    "    end = time.time()\n",
    "    \n",
    "print(\"Duration: \", end - start, \"Count: \", len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
